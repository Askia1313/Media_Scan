"""
Gestionnaire principal de scraping avec fallback automatique
"""

from typing import List, Tuple
from urllib.parse import urlparse

from database.db_manager import DatabaseManager
from database.models import Article
from .smart_html_scraper import SmartHTMLScraper


class ScraperManager:
    """Gestionnaire de scraping HTML intelligent"""
    
    def __init__(self, db_manager: DatabaseManager):
        """
        Initialise le gestionnaire
        
        Args:
            db_manager: Instance de DatabaseManager
        """
        self.db = db_manager
    
    def scrape_site(self, url: str, days: int = 30) -> Tuple[int, str, str]:
        """
        Scraper un site avec le scraper HTML intelligent
        
        Args:
            url: URL du site Ã  scraper
            days: Nombre de jours Ã  rÃ©cupÃ©rer
        
        Returns:
            Tuple (nombre d'articles, mÃ©thode utilisÃ©e, message)
        """
        # Nettoyer l'URL
        url = url.strip().rstrip('/')
        
        # Extraire le nom du domaine pour le nom du mÃ©dia
        domain = urlparse(url).netloc
        media_name = domain.replace('www.', '').split('.')[0].capitalize()
        
        print(f"\n{'='*60}")
        print(f"ğŸ¯ Scraping: {media_name} ({url})")
        print(f"{'='*60}\n")
        
        try:
            # Utiliser le scraper HTML intelligent
            scraper = SmartHTMLScraper(url)
            
            # Ajouter/mettre Ã  jour le mÃ©dia
            media_id = self.db.add_media(media_name, url, 'html')
            
            # Scraper les articles
            articles = scraper.scrape(media_id, days=days, max_articles=100)
            
            # Sauvegarder en base
            saved_count = self._save_articles(articles)
            
            # Mettre Ã  jour la date de derniÃ¨re collecte
            self.db.update_media_last_scrape(media_id)
            
            # Logger
            self.db.add_scraping_log(
                media_id=media_id,
                status='success' if saved_count > 0 else 'partial',
                methode='html_scraping',
                articles_collectes=saved_count,
                message=f"{saved_count} articles collectÃ©s via scraping HTML"
            )
            
            return saved_count, 'html_scraping', f"âœ… {saved_count} articles collectÃ©s via scraping HTML"
        
        except Exception as e:
            error_msg = f"âŒ Erreur scraping: {e}"
            print(error_msg)
            
            # Logger l'erreur
            media_id = self.db.add_media(media_name, url, 'unknown')
            self.db.add_scraping_log(
                media_id=media_id,
                status='error',
                methode='html_scraping',
                articles_collectes=0,
                message=str(e)
            )
            
            return 0, 'error', error_msg
    
    def _save_articles(self, articles: List[Article]) -> int:
        """
        Sauvegarder les articles en base de donnÃ©es
        
        Args:
            articles: Liste d'articles Ã  sauvegarder
        
        Returns:
            Nombre d'articles sauvegardÃ©s (nouveaux uniquement)
        """
        saved_count = 0
        duplicate_count = 0
        
        for article in articles:
            # VÃ©rifier si l'article existe dÃ©jÃ 
            if not self.db.article_exists(article.url):
                article_id = self.db.add_article(article)
                if article_id:
                    saved_count += 1
            else:
                duplicate_count += 1
        
        if duplicate_count > 0:
            print(f"   ğŸ’¾ {saved_count} nouveaux articles, {duplicate_count} doublons ignorÃ©s")
        
        return saved_count
    
    def scrape_all_sites(self, sites_file: str = 'sites.txt', days: int = 30) -> dict:
        """
        Scraper tous les sites listÃ©s dans un fichier
        
        Args:
            sites_file: Chemin vers le fichier contenant les URLs
            days: Nombre de jours Ã  rÃ©cupÃ©rer
        
        Returns:
            Dictionnaire avec les statistiques
        """
        print("\n" + "="*60)
        print("ğŸš€ MÃ‰DIA-SCAN - Collecte Multi-Sites")
        print("="*60)
        
        # Lire le fichier des sites
        try:
            with open(sites_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except FileNotFoundError:
            print(f"âŒ Fichier {sites_file} non trouvÃ©")
            return {}
        
        # Filtrer les lignes (ignorer commentaires et lignes vides)
        urls = []
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#'):
                urls.append(line)
        
        print(f"\nğŸ“‹ {len(urls)} sites Ã  scraper")
        print(f"ğŸ“… PÃ©riode: {days} derniers jours\n")
        
        # Statistiques
        stats = {
            'total_sites': len(urls),
            'success': 0,
            'errors': 0,
            'total_articles': 0,
            'by_method': {
                'html_scraping': 0,
                'error': 0
            },
            'details': []
        }
        
        # Scraper chaque site
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] Traitement de {url}...")
            
            count, method, message = self.scrape_site(url, days=days)
            
            if count > 0:
                stats['success'] += 1
                stats['total_articles'] += count
            else:
                stats['errors'] += 1
            
            stats['by_method'][method] = stats['by_method'].get(method, 0) + count
            
            stats['details'].append({
                'url': url,
                'articles': count,
                'method': method,
                'message': message
            })
            
            print(message)
        
        # Afficher le rÃ©sumÃ©
        self._print_summary(stats)
        
        return stats
    
    def _print_summary(self, stats: dict):
        """Afficher le rÃ©sumÃ© de la collecte"""
        print("\n" + "="*60)
        print("ğŸ“Š RÃ‰SUMÃ‰ DE LA COLLECTE")
        print("="*60)
        print(f"\nâœ… Sites traitÃ©s: {stats['total_sites']}")
        print(f"   â€¢ SuccÃ¨s: {stats['success']}")
        print(f"   â€¢ Erreurs: {stats['errors']}")
        print(f"\nğŸ“° Total articles collectÃ©s: {stats['total_articles']}")
        print(f"\nğŸ”§ Par mÃ©thode:")
        print(f"   â€¢ HTML Scraping: {stats['by_method'].get('html_scraping', 0)} articles")
        
        print(f"\nğŸ“‹ DÃ©tails par site:")
        for detail in stats['details']:
            status = "âœ…" if detail['articles'] > 0 else "âŒ"
            print(f"   {status} {detail['url']}: {detail['articles']} articles ({detail['method']})")
        
        print("\n" + "="*60)
