"""
Module de d√©tection de contenus sensibles avec Ollama
D√©tecte : incitation √† la haine, fake news, discours toxique
"""

import requests
import json
from typing import Dict, List, Optional, Tuple
from datetime import datetime


class ContentModerator:
    """
    Analyseur de contenus sensibles utilisant Ollama
    """
    
    def __init__(self, ollama_url: str = "http://localhost:11434", model: str = "mistral:latest"):
        """
        Initialise le mod√©rateur de contenu
        
        Args:
            ollama_url: URL de l'API Ollama
            model: Mod√®le Ollama √† utiliser
        """
        self.ollama_url = ollama_url
        self.model = model
        self.api_endpoint = f"{ollama_url}/api/generate"
    
    def check_ollama_status(self) -> bool:
        """
        V√©rifie si Ollama est disponible
        
        Returns:
            True si Ollama est disponible, False sinon
        """
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def _call_ollama(self, prompt: str, max_tokens: int = 500) -> str:
        """
        Appelle l'API Ollama
        
        Args:
            prompt: Le prompt √† envoyer
            max_tokens: Nombre maximum de tokens
            
        Returns:
            R√©ponse du mod√®le
        """
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,  # Faible temp√©rature pour plus de coh√©rence
                    "num_predict": max_tokens
                }
            }
            
            response = requests.post(
                self.api_endpoint,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '').strip()
            else:
                print(f"‚ùå Erreur Ollama: {response.status_code}")
                return ""
                
        except Exception as e:
            print(f"‚ùå Erreur lors de l'appel √† Ollama: {e}")
            return ""
    
    def analyze_toxicity(self, text: str) -> Dict:
        """
        Analyse la toxicit√© d'un contenu
        
        Args:
            text: Texte √† analyser
            
        Returns:
            Dict avec score de toxicit√© et d√©tails
        """
        prompt = f"""Tu es un mod√©rateur de contenu pour un r√©gulateur des m√©dias au Burkina Faso.

IMPORTANT: Distingue entre:
- RAPPORTER/INFORMER sur des faits (m√™me graves) = L√âGITIME, score faible
- PROMOUVOIR/INCITER √† la haine ou violence = PROBL√âMATIQUE, score √©lev√©

Texte √† analyser: "{text}"

√âvalue UNIQUEMENT si l'auteur INCITE ou PROMEUT (note de 0 √† 10):
1. Incitation √† la haine (contre un groupe ethnique, religieux, etc.)
2. Appel √† la violence ou discours agressif
3. Insultes ou langage offensant direct
4. Discrimination active

Un article qui RAPPORTE des faits (m√™me violents) sans les promouvoir doit avoir un score FAIBLE.

R√©ponds UNIQUEMENT au format JSON suivant:
{{
    "est_toxique": true/false,
    "score_toxicite": 0-10,
    "contexte": "informatif/promotionnel"
}}"""

        response = self._call_ollama(prompt)
        
        try:
            # Extraire le JSON de la r√©ponse
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end > json_start:
                json_str = response[json_start:json_end]
                result = json.loads(json_str)
                return result
            else:
                return self._default_toxicity_result()
        except json.JSONDecodeError:
            print(f"‚ö†Ô∏è Impossible de parser la r√©ponse JSON: {response}")
            return self._default_toxicity_result()
    
    def analyze_misinformation(self, text: str) -> Dict:
        """
        Analyse si le contenu contient de la d√©sinformation
        
        Args:
            text: Texte √† analyser
            
        Returns:
            Dict avec score de d√©sinformation et d√©tails
        """
        prompt = f"""Tu es un mod√©rateur de contenu pour un r√©gulateur des m√©dias au Burkina Faso.

IMPORTANT: Distingue entre:
- Article journalistique factuel avec sources = L√âGITIME, score faible
- Affirmations fausses pr√©sent√©es comme vraies = PROBL√âMATIQUE, score √©lev√©

Texte √† analyser: "{text}"

√âvalue UNIQUEMENT si le contenu PROPAGE de fausses informations (note de 0 √† 10):
1. Affirmations manifestement fausses ou non v√©rifiables
2. Manipulation √©vidente de faits
3. Th√©ories du complot sans fondement
4. Propagande mensong√®re

Un article qui cite des sources officielles ou rapporte des faits v√©rifiables doit avoir un score FAIBLE.

R√©ponds UNIQUEMENT au format JSON suivant:
{{
    "est_desinformation": true/false,
    "score_desinformation": 0-10,
    "sources_citees": true/false
}}"""

        response = self._call_ollama(prompt)
        
        try:
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end > json_start:
                json_str = response[json_start:json_end]
                result = json.loads(json_str)
                return result
            else:
                return self._default_misinformation_result()
        except json.JSONDecodeError:
            print(f"‚ö†Ô∏è Impossible de parser la r√©ponse JSON: {response}")
            return self._default_misinformation_result()
    
    def analyze_sensitivity(self, text: str) -> Dict:
        """
        Analyse la sensibilit√© globale du contenu
        
        Args:
            text: Texte √† analyser
            
        Returns:
            Dict avec niveau de sensibilit√© et cat√©gories
        """
        prompt = f"""Tu es un mod√©rateur de contenu pour un r√©gulateur des m√©dias au Burkina Faso.

IMPORTANT: 
- Un article qui INFORME sur des sujets sensibles de mani√®re FACTUELLE = sensibilit√© FAIBLE/MOYENNE
- Un article qui EXPLOITE ou SENSATIONNALISE de mani√®re irresponsable = sensibilit√© √âLEV√âE/CRITIQUE

Texte √† analyser: "{text}"

√âvalue si le contenu aborde des sujets sensibles de mani√®re IRRESPONSABLE:
1. Violence ou conflit arm√© (traitement sensationnaliste)
2. Terrorisme (glorification ou d√©tails excessifs)
3. Politique controvers√©e (partialit√© extr√™me)
4. Religion sensible (propos offensants)
5. Sant√© publique (panique inutile)

Un reportage factuel et √©quilibr√© sur ces sujets doit avoir un score FAIBLE.

R√©ponds UNIQUEMENT au format JSON suivant:
{{
    "est_sensible": true/false,
    "score_sensibilite": 0-10,
    "traitement": "factuel/sensationnaliste"
}}"""

        response = self._call_ollama(prompt)
        
        try:
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end > json_start:
                json_str = response[json_start:json_end]
                result = json.loads(json_str)
                return result
            else:
                return self._default_sensitivity_result()
        except json.JSONDecodeError:
            print(f"‚ö†Ô∏è Impossible de parser la r√©ponse JSON: {response}")
            return self._default_sensitivity_result()
    
    def analyze_content(self, text: str, content_type: str = "article") -> Dict:
        """
        Analyse compl√®te d'un contenu en un seul appel IA
        
        Args:
            text: Texte √† analyser
            content_type: Type de contenu (article, facebook_post, tweet)
            
        Returns:
            Dict avec toutes les analyses
        """
        if not text or len(text.strip()) < 10:
            return self._default_analysis_result()
        
        print(f"üîç Analyse du contenu ({content_type})...")
        
        # Limiter la taille du texte pour l'analyse
        text_sample = text[:2000] if len(text) > 2000 else text
        
        # Analyse unifi√©e avec d√©cision de l'IA sur le type principal
        prompt = f"""Tu es un mod√©rateur de contenu pour un r√©gulateur des m√©dias au Burkina Faso.

Analyse ce texte et d√©termine s'il contient du contenu probl√©matique.

IMPORTANT: Distingue entre:
- RAPPORTER/INFORMER sur des faits = L√âGITIME, scores faibles
- PROMOUVOIR/INCITER = PROBL√âMATIQUE, scores √©lev√©s

Texte: "{text_sample}"

√âvalue ces 3 aspects (scores 0-10):
1. Toxicit√©: Incitation √† la haine, violence, insultes, discrimination
2. D√©sinformation: Fausses informations, manipulation, th√©ories du complot
3. Sensibilit√©: Traitement irresponsable de sujets sensibles

Puis D√âCIDE quel est le probl√®me PRINCIPAL (si aucun, mets "none"):
- "toxicity" si c'est principalement du discours de haine/violence
- "misinformation" si c'est principalement de la d√©sinformation
- "sensitivity" si c'est principalement un traitement sensationnaliste
- "none" si le contenu est acceptable

R√©ponds UNIQUEMENT au format JSON:
{{
    "toxicity_score": 0-10,
    "misinformation_score": 0-10,
    "sensitivity_score": 0-10,
    "primary_issue": "toxicity/misinformation/sensitivity/none",
    "contexte": "informatif/promotionnel",
    "sources_citees": true/false,
    "traitement": "factuel/sensationnaliste"
}}"""

        response = self._call_ollama(prompt, max_tokens=200)
        
        try:
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end > json_start:
                json_str = response[json_start:json_end]
                result = json.loads(json_str)
                
                # Construire les d√©tails
                toxicity = {
                    'est_toxique': result.get('toxicity_score', 0) >= 6,
                    'score_toxicite': result.get('toxicity_score', 0),
                    'contexte': result.get('contexte', 'informatif')
                }
                
                misinformation = {
                    'est_desinformation': result.get('misinformation_score', 0) >= 6,
                    'score_desinformation': result.get('misinformation_score', 0),
                    'sources_citees': result.get('sources_citees', False)
                }
                
                sensitivity = {
                    'est_sensible': result.get('sensitivity_score', 0) >= 6,
                    'score_sensibilite': result.get('sensitivity_score', 0),
                    'traitement': result.get('traitement', 'factuel')
                }
                
                # Calcul du score de risque
                risk_score = (
                    result.get('toxicity_score', 0) * 0.4 +
                    result.get('misinformation_score', 0) * 0.4 +
                    result.get('sensitivity_score', 0) * 0.2
                )
                
                risk_level = self._determine_risk_level(risk_score)
                
                should_flag = (
                    risk_score >= 7.0 or
                    result.get('toxicity_score', 0) >= 8.0 or
                    result.get('misinformation_score', 0) >= 8.0
                )
                
                return {
                    'content_type': content_type,
                    'analyzed_at': datetime.now().isoformat(),
                    'toxicity': toxicity,
                    'misinformation': misinformation,
                    'sensitivity': sensitivity,
                    'risk_score': round(risk_score, 2),
                    'risk_level': risk_level,
                    'should_flag': should_flag,
                    'primary_issue': result.get('primary_issue', 'none'),
                    'text_length': len(text)
                }
            else:
                return self._default_analysis_result()
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur parsing: {e}")
            return self._default_analysis_result()
    
    def _calculate_risk_score(self, toxicity: Dict, misinformation: Dict, sensitivity: Dict) -> float:
        """
        Calcule le score de risque global
        
        Args:
            toxicity: R√©sultat de l'analyse de toxicit√©
            misinformation: R√©sultat de l'analyse de d√©sinformation
            sensitivity: R√©sultat de l'analyse de sensibilit√©
            
        Returns:
            Score de risque (0-10)
        """
        # Pond√©ration: toxicit√© 40%, d√©sinformation 40%, sensibilit√© 20%
        toxicity_score = toxicity.get('score_toxicite', 0)
        misinfo_score = misinformation.get('score_desinformation', 0)
        sensitivity_score = sensitivity.get('score_sensibilite', 0)
        
        risk_score = (
            toxicity_score * 0.4 +
            misinfo_score * 0.4 +
            sensitivity_score * 0.2
        )
        
        return risk_score
    
    def _determine_risk_level(self, risk_score: float) -> str:
        """
        D√©termine le niveau de risque
        
        Args:
            risk_score: Score de risque (0-10)
            
        Returns:
            Niveau de risque
        """
        if risk_score >= 8:
            return "üî¥ CRITIQUE"
        elif risk_score >= 6:
            return "üü† √âLEV√â"
        elif risk_score >= 4:
            return "üü° MOYEN"
        elif risk_score >= 2:
            return "üü¢ FAIBLE"
        else:
            return "‚úÖ MINIMAL"
    
    def _determine_primary_issue(self, toxicity: Dict, misinformation: Dict, sensitivity: Dict) -> str:
        """
        D√©termine le type principal de probl√®me d√©tect√©
        
        Args:
            toxicity: R√©sultat de l'analyse de toxicit√©
            misinformation: R√©sultat de l'analyse de d√©sinformation
            sensitivity: R√©sultat de l'analyse de sensibilit√©
            
        Returns:
            Type principal: 'toxicity', 'misinformation', 'sensitivity', ou 'none'
        """
        tox_score = toxicity.get('score_toxicite', 0)
        mis_score = misinformation.get('score_desinformation', 0)
        sens_score = sensitivity.get('score_sensibilite', 0)
        
        # Si aucun score significatif
        if max(tox_score, mis_score, sens_score) < 3:
            return 'none'
        
        # Retourner le type avec le score le plus √©lev√©
        max_score = max(tox_score, mis_score, sens_score)
        
        if max_score == tox_score:
            return 'toxicity'
        elif max_score == mis_score:
            return 'misinformation'
        else:
            return 'sensitivity'
    
    def _default_toxicity_result(self) -> Dict:
        """R√©sultat par d√©faut pour l'analyse de toxicit√©"""
        return {
            'est_toxique': False,
            'score_toxicite': 0,
            'incitation_haine': 0,
            'violence': 0,
            'insultes': 0,
            'discrimination': 0,
            'raison': 'Analyse non disponible'
        }
    
    def _default_misinformation_result(self) -> Dict:
        """R√©sultat par d√©faut pour l'analyse de d√©sinformation"""
        return {
            'est_desinformation': False,
            'score_desinformation': 0,
            'affirmations_non_verifiees': 0,
            'manipulation_faits': 0,
            'theorie_complot': 0,
            'propagande': 0,
            'raison': 'Analyse non disponible',
            'elements_suspects': []
        }
    
    def _default_sensitivity_result(self) -> Dict:
        """R√©sultat par d√©faut pour l'analyse de sensibilit√©"""
        return {
            'est_sensible': False,
            'niveau_sensibilite': 'faible',
            'score_sensibilite': 0,
            'categories_sensibles': [],
            'raison': 'Analyse non disponible'
        }
    
    def _default_analysis_result(self) -> Dict:
        """R√©sultat par d√©faut pour une analyse compl√®te"""
        return {
            'content_type': 'unknown',
            'analyzed_at': datetime.now().isoformat(),
            'toxicity': self._default_toxicity_result(),
            'misinformation': self._default_misinformation_result(),
            'sensitivity': self._default_sensitivity_result(),
            'risk_score': 0,
            'risk_level': '‚úÖ MINIMAL',
            'should_flag': False,
            'text_length': 0
        }
    
    def test_connection(self) -> bool:
        """
        Teste la connexion √† Ollama
        
        Returns:
            True si la connexion fonctionne
        """
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                print(f"‚úÖ Connexion √† Ollama r√©ussie")
                print(f"üì¶ Mod√®les disponibles: {[m['name'] for m in models]}")
                return True
            else:
                print(f"‚ùå Erreur de connexion √† Ollama: {response.status_code}")
                return False
        except Exception as e:
            print(f"‚ùå Impossible de se connecter √† Ollama: {e}")
            print(f"üí° Assurez-vous qu'Ollama est lanc√©: ollama serve")
            return False


# Fonction utilitaire pour analyser rapidement un texte
def analyze_text(text: str, content_type: str = "article") -> Dict:
    """
    Fonction utilitaire pour analyser un texte
    
    Args:
        text: Texte √† analyser
        content_type: Type de contenu
        
    Returns:
        R√©sultat de l'analyse
    """
    moderator = ContentModerator()
    return moderator.analyze_content(text, content_type)
