# ğŸš€ Guide d'Utilisation Rapide - MÃ‰DIA-SCAN

## ğŸ“ Configuration Initiale

### 1. Configurer les sites Ã  scraper

Ã‰ditez le fichier `sites.txt` et ajoutez les URLs des mÃ©dias :

```txt
https://lefaso.net
https://www.sidwaya.info
https://www.fasopresse.net
https://www.lobservateur.bf
https://www.aib.media
```

### 2. VÃ©rifier les dÃ©pendances

```bash
pip install requests beautifulsoup4 lxml
```

---

## ğŸ¯ Utilisation Basique

### Scraper tous les sites (30 derniers jours)

```bash
python run_scraper.py
```

### Scraper avec une pÃ©riode personnalisÃ©e

```bash
# 7 derniers jours
python run_scraper.py --days 7

# 60 derniers jours
python run_scraper.py --days 60
```

### Scraper un seul site

```bash
python run_scraper.py --url https://lefaso.net
```

### Afficher les statistiques

```bash
python run_scraper.py --stats
```

---

## ğŸ§ª Tester le SystÃ¨me

### Lancer les tests

```bash
python test_scraper.py
```

**Tests effectuÃ©s :**
1. âœ… DÃ©tection WordPress sur plusieurs sites
2. âœ… Scraping WordPress avec API
3. âœ… Scraping HTML (fallback)
4. âœ… Sauvegarde en base de donnÃ©es
5. âœ… Statistiques

---

## ğŸ“Š Comprendre les RÃ©sultats

### Sortie du scraping

```
============================================================
ğŸ¯ Scraping: Lefaso (https://lefaso.net)
============================================================

ğŸ“¡ Tentative 1: API WordPress...
âœ… Site WordPress dÃ©tectÃ©!
ğŸ“¡ RÃ©cupÃ©ration articles WordPress depuis https://lefaso.net...
   Page 1: 100 articles rÃ©cupÃ©rÃ©s
   Page 2: 45 articles rÃ©cupÃ©rÃ©s
âœ… Total: 145 articles rÃ©cupÃ©rÃ©s
âœ… 145 articles collectÃ©s via API WordPress
```

**Signification :**
- âœ… **WordPress dÃ©tectÃ©** : Le site utilise WordPress, donnÃ©es de qualitÃ©
- ğŸ“¡ **Pages rÃ©cupÃ©rÃ©es** : Nombre de pages d'articles scrapÃ©es
- âœ… **Total** : Nombre total d'articles collectÃ©s

### MÃ©thodes de scraping

| MÃ©thode | Description | QualitÃ© |
|---------|-------------|---------|
| `wordpress_api` | API WordPress REST | â­â­â­â­â­ Excellente |
| `html_scraping` | Scraping HTML gÃ©nÃ©rique | â­â­â­ Bonne |
| `error` | Ã‰chec du scraping | âŒ Aucune donnÃ©e |

---

## ğŸ—„ï¸ AccÃ©der aux DonnÃ©es

### Base de donnÃ©es SQLite

Les donnÃ©es sont sauvegardÃ©es dans `data/media_scan.db`

### Visualiser avec Python

```python
from database.db_manager import DatabaseManager

# Initialiser
db = DatabaseManager()

# RÃ©cupÃ©rer les articles rÃ©cents
articles = db.get_recent_articles(days=30, limit=10)

for article in articles:
    print(f"{article.titre}")
    print(f"URL: {article.url}")
    print(f"Date: {article.date_publication}")
    print(f"Contenu: {article.contenu[:200]}...")
    print("-" * 60)
```

### Visualiser avec SQLite Browser

1. TÃ©lÃ©chargez [DB Browser for SQLite](https://sqlitebrowser.org/)
2. Ouvrez `data/media_scan.db`
3. Explorez les tables `articles` et `medias`

---

## ğŸ”§ Personnalisation

### Modifier la pÃ©riode de collecte

Dans `run_scraper.py`, changez la valeur par dÃ©faut :

```python
parser.add_argument(
    '--days',
    type=int,
    default=30,  # Changez ici
    help='Nombre de jours Ã  rÃ©cupÃ©rer'
)
```

### Ajouter un nouveau site

1. Ajoutez l'URL dans `sites.txt`
2. Lancez : `python run_scraper.py`

Le systÃ¨me dÃ©tecte automatiquement WordPress ou utilise HTML.

### Modifier les sÃ©lecteurs HTML

Si un site spÃ©cifique ne fonctionne pas, Ã©ditez `scrapers/html_scraper.py` :

```python
def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
    selectors = [
        'h1.mon-selecteur-custom',  # Ajoutez ici
        'h1.entry-title',
        # ...
    ]
```

---

## ğŸ“ˆ Automatisation

### Scraping quotidien (Windows)

CrÃ©ez un fichier `scrape_daily.bat` :

```batch
@echo off
cd C:\Users\DarkSide\Desktop\Media_Scanne\backend\django_back
python run_scraper.py --days 1
```

Ajoutez-le au Planificateur de tÃ¢ches Windows pour l'exÃ©cuter tous les jours.

### Scraping quotidien (Linux/Mac)

Ajoutez au crontab :

```bash
# Tous les jours Ã  6h du matin
0 6 * * * cd /path/to/django_back && python run_scraper.py --days 1
```

---

## ğŸ› RÃ©solution de ProblÃ¨mes

### "Site non WordPress ou API non accessible"

**Normal** : Le systÃ¨me bascule automatiquement sur HTML scraping.

### "Aucun article trouvÃ©"

**VÃ©rifiez :**
1. Connexion internet
2. URL correcte dans `sites.txt`
3. Site accessible dans un navigateur

**Solution :** Testez avec un seul site :
```bash
python run_scraper.py --url https://lefaso.net
```

### "Database is locked"

**Cause :** Un autre processus utilise la base de donnÃ©es.

**Solution :** Attendez la fin du scraping en cours.

### Erreur de parsing HTML

**Cause :** Structure HTML du site non reconnue.

**Solution :** Modifiez les sÃ©lecteurs dans `scrapers/html_scraper.py`

---

## ğŸ’¡ Conseils

### Performance

- âœ… Scrapez rÃ©guliÃ¨rement (ex: tous les jours) plutÃ´t que tout d'un coup
- âœ… Limitez la pÃ©riode avec `--days` pour aller plus vite
- âœ… Le systÃ¨me inclut des pauses automatiques pour ne pas surcharger les serveurs

### QualitÃ© des donnÃ©es

- â­ **WordPress API** : DonnÃ©es structurÃ©es, complÃ¨tes et fiables
- â­ **HTML Scraping** : DonnÃ©es moins structurÃ©es, peut nÃ©cessiter des ajustements

### Stockage

- La base de donnÃ©es SQLite est lÃ©gÃ¨re (~1 Mo pour 1000 articles)
- Nettoyez les anciens articles si nÃ©cessaire :

```python
from database.db_manager import DatabaseManager
db = DatabaseManager()
db.clear_old_articles(days=90)  # Supprimer articles > 90 jours
```

---

## ğŸ“ Commandes Utiles

```bash
# Scraping standard
python run_scraper.py

# Scraping avec pÃ©riode personnalisÃ©e
python run_scraper.py --days 7

# Scraper un seul site
python run_scraper.py --url https://lefaso.net

# Statistiques
python run_scraper.py --stats

# Tests
python test_scraper.py

# Aide
python run_scraper.py --help
```

---

## âœ… Checklist de VÃ©rification

Avant de lancer le scraping :

- [ ] Python 3.9+ installÃ©
- [ ] DÃ©pendances installÃ©es (`pip install -r requirements.txt`)
- [ ] Fichier `sites.txt` configurÃ© avec les URLs
- [ ] Connexion internet active
- [ ] Tests rÃ©ussis (`python test_scraper.py`)

**PrÃªt Ã  scraper ! ğŸš€**
