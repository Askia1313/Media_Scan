"""
Script de test pour la modÃ©ration de contenu
"""

from analysis.content_moderator import ContentModerator


def test_toxicity():
    """Test de dÃ©tection de toxicitÃ©"""
    print("\nğŸ§ª Test de dÃ©tection de toxicitÃ©")
    print("=" * 80)
    
    moderator = ContentModerator()
    
    # Test 1: Contenu neutre
    text1 = "Le prÃ©sident a annoncÃ© de nouvelles mesures Ã©conomiques pour soutenir les entreprises."
    result1 = moderator.analyze_toxicity(text1)
    print(f"\nğŸ“ Texte neutre:")
    print(f"   Toxique: {result1['est_toxique']}")
    print(f"   Score: {result1['score_toxicite']}/10")
    
    # Test 2: Contenu potentiellement toxique
    text2 = "Ces gens sont tous des menteurs et des voleurs qui dÃ©truisent notre pays!"
    result2 = moderator.analyze_toxicity(text2)
    print(f"\nğŸ“ Texte potentiellement toxique:")
    print(f"   Toxique: {result2['est_toxique']}")
    print(f"   Score: {result2['score_toxicite']}/10")
    print(f"   Raison: {result2['raison']}")


def test_misinformation():
    """Test de dÃ©tection de dÃ©sinformation"""
    print("\nğŸ§ª Test de dÃ©tection de dÃ©sinformation")
    print("=" * 80)
    
    moderator = ContentModerator()
    
    # Test 1: Information factuelle
    text1 = "Selon les donnÃ©es officielles, l'inflation a augmentÃ© de 2% ce trimestre."
    result1 = moderator.analyze_misinformation(text1)
    print(f"\nğŸ“ Information factuelle:")
    print(f"   DÃ©sinformation: {result1['est_desinformation']}")
    print(f"   Score: {result1['score_desinformation']}/10")
    
    # Test 2: Affirmation non vÃ©rifiÃ©e
    text2 = "Des sources secrÃ¨tes rÃ©vÃ¨lent que le gouvernement cache la vÃ©ritÃ© sur l'Ã©pidÃ©mie!"
    result2 = moderator.analyze_misinformation(text2)
    print(f"\nğŸ“ Affirmation non vÃ©rifiÃ©e:")
    print(f"   DÃ©sinformation: {result2['est_desinformation']}")
    print(f"   Score: {result2['score_desinformation']}/10")
    print(f"   Raison: {result2['raison']}")


def test_sensitivity():
    """Test de dÃ©tection de sensibilitÃ©"""
    print("\nğŸ§ª Test de dÃ©tection de sensibilitÃ©")
    print("=" * 80)
    
    moderator = ContentModerator()
    
    # Test 1: Contenu non sensible
    text1 = "Le festival culturel aura lieu ce weekend avec de nombreux artistes locaux."
    result1 = moderator.analyze_sensitivity(text1)
    print(f"\nğŸ“ Contenu non sensible:")
    print(f"   Sensible: {result1['est_sensible']}")
    print(f"   Niveau: {result1['niveau_sensibilite']}")
    print(f"   Score: {result1['score_sensibilite']}/10")
    
    # Test 2: Contenu sensible
    text2 = "Nouvelle attaque terroriste dans le nord du pays, plusieurs victimes signalÃ©es."
    result2 = moderator.analyze_sensitivity(text2)
    print(f"\nğŸ“ Contenu sensible:")
    print(f"   Sensible: {result2['est_sensible']}")
    print(f"   Niveau: {result2['niveau_sensibilite']}")
    print(f"   Score: {result2['score_sensibilite']}/10")
    print(f"   CatÃ©gories: {result2['categories_sensibles']}")


def test_full_analysis():
    """Test d'analyse complÃ¨te"""
    print("\nğŸ§ª Test d'analyse complÃ¨te")
    print("=" * 80)
    
    moderator = ContentModerator()
    
    # Texte d'exemple
    text = """
    Le gouvernement a annoncÃ© de nouvelles mesures de sÃ©curitÃ© suite aux rÃ©cents 
    Ã©vÃ©nements dans la rÃ©gion du Sahel. Ces mesures visent Ã  renforcer la protection 
    des populations civiles face aux menaces terroristes.
    """
    
    result = moderator.analyze_content(text, 'article')
    
    print(f"\nğŸ“ Texte analysÃ©:")
    print(f"   Type: {result['content_type']}")
    print(f"   Longueur: {result['text_length']} caractÃ¨res")
    print(f"\nğŸ“Š RÃ©sultats:")
    print(f"   Score de risque: {result['risk_score']}/10")
    print(f"   Niveau de risque: {result['risk_level']}")
    print(f"   Ã€ signaler: {result['should_flag']}")
    print(f"\nğŸ” DÃ©tails:")
    print(f"   Toxique: {result['toxicity']['est_toxique']} (Score: {result['toxicity']['score_toxicite']})")
    print(f"   DÃ©sinformation: {result['misinformation']['est_desinformation']} (Score: {result['misinformation']['score_desinformation']})")
    print(f"   Sensible: {result['sensitivity']['est_sensible']} (Score: {result['sensitivity']['score_sensibilite']})")


def main():
    """Fonction principale"""
    print("\nğŸ›¡ï¸ Tests de modÃ©ration de contenu avec Ollama")
    print("=" * 80)
    
    # Tester la connexion
    moderator = ContentModerator()
    if not moderator.test_connection():
        print("\nâŒ Impossible de se connecter Ã  Ollama")
        print("ğŸ’¡ Lancez Ollama avec: ollama serve")
        print("ğŸ’¡ TÃ©lÃ©chargez le modÃ¨le avec: ollama pull llama3.2")
        return
    
    # Lancer les tests
    try:
        test_toxicity()
        test_misinformation()
        test_sensitivity()
        test_full_analysis()
        
        print("\nâœ… Tous les tests sont terminÃ©s")
        
    except Exception as e:
        print(f"\nâŒ Erreur lors des tests: {e}")


if __name__ == "__main__":
    main()
