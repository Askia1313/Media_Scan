#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script principal pour lancer le scraping des mÃ©dias
"""

import sys
import argparse
from pathlib import Path

# Ajouter le dossier parent au PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent))

from database.db_manager import DatabaseManager
from scrapers.scraper_manager import ScraperManager


def main():
    """Fonction principale"""
    parser = argparse.ArgumentParser(
        description='MÃ‰DIA-SCAN - Scraper de mÃ©dias burkinabÃ¨'
    )
    
    parser.add_argument(
        '--sites-file',
        type=str,
        default='sites.txt',
        help='Fichier contenant les URLs des sites (dÃ©faut: sites.txt)'
    )
    
    parser.add_argument(
        '--days',
        type=int,
        default=30,
        help='Nombre de jours Ã  rÃ©cupÃ©rer (dÃ©faut: 30)'
    )
    
    parser.add_argument(
        '--db-path',
        type=str,
        default='data/media_scan.db',
        help='Chemin vers la base de donnÃ©es (dÃ©faut: data/media_scan.db)'
    )
    
    parser.add_argument(
        '--url',
        type=str,
        help='Scraper un seul site (URL)'
    )
    
    parser.add_argument(
        '--stats',
        action='store_true',
        help='Afficher les statistiques de la base de donnÃ©es'
    )
    
    args = parser.parse_args()
    
    # Initialiser la base de donnÃ©es
    print("ğŸ”§ Initialisation de la base de donnÃ©es...")
    db = DatabaseManager(db_path=args.db_path)
    
    # Afficher les stats si demandÃ©
    if args.stats:
        print_stats(db)
        return
    
    # Initialiser le gestionnaire de scraping
    manager = ScraperManager(db)
    
    # Scraper un seul site ou tous les sites
    if args.url:
        # Scraper un seul site
        count, method, message = manager.scrape_site(args.url, days=args.days)
        print(f"\n{message}")
    else:
        # Scraper tous les sites du fichier
        stats = manager.scrape_all_sites(
            sites_file=args.sites_file,
            days=args.days
        )
        
        # Afficher les stats finales
        print("\nğŸ“Š Statistiques de la base de donnÃ©es:")
        print_stats(db)


def print_stats(db: DatabaseManager):
    """Afficher les statistiques de la base de donnÃ©es"""
    stats = db.get_scraping_stats()
    
    print("\n" + "="*60)
    print("ğŸ“Š STATISTIQUES DE LA BASE DE DONNÃ‰ES")
    print("="*60)
    
    print(f"\nğŸ“° Total articles: {stats['total_articles']}")
    
    print(f"\nğŸ“º Articles par mÃ©dia:")
    for media, count in stats['articles_par_media'].items():
        print(f"   â€¢ {media}: {count} articles")
    
    print(f"\nğŸ”§ Articles par source:")
    for source, count in stats['articles_par_source'].items():
        print(f"   â€¢ {source}: {count} articles")
    
    if stats['derniers_logs']:
        print(f"\nğŸ“‹ Derniers logs de scraping:")
        for log in stats['derniers_logs'][:5]:
            status_icon = "âœ…" if log['status'] == 'success' else "âŒ"
            print(f"   {status_icon} {log['media_nom']}: {log['articles_collectes']} articles ({log['methode']})")
            if log['message']:
                print(f"      â†’ {log['message']}")
    
    print("\n" + "="*60)


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Interruption par l'utilisateur")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ Erreur: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
