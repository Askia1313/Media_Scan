# ğŸ’» Exemples de Code - MÃ‰DIA-SCAN

## ğŸ“‹ Table des MatiÃ¨res

1. [Utilisation Basique](#utilisation-basique)
2. [Scraping PersonnalisÃ©](#scraping-personnalisÃ©)
3. [AccÃ¨s aux DonnÃ©es](#accÃ¨s-aux-donnÃ©es)
4. [Statistiques](#statistiques)
5. [Automatisation](#automatisation)

---

## ğŸš€ Utilisation Basique

### Scraper Tous les Sites

```python
from database.db_manager import DatabaseManager
from scrapers.scraper_manager import ScraperManager

# Initialiser
db = DatabaseManager()
manager = ScraperManager(db)

# Scraper tous les sites du fichier sites.txt
stats = manager.scrape_all_sites(
    sites_file='sites.txt',
    days=30  # 30 derniers jours
)

print(f"Total articles collectÃ©s: {stats['total_articles']}")
```

### Scraper un Seul Site

```python
from database.db_manager import DatabaseManager
from scrapers.scraper_manager import ScraperManager

db = DatabaseManager()
manager = ScraperManager(db)

# Scraper Lefaso.net
count, method, message = manager.scrape_site(
    url='https://lefaso.net',
    days=30
)

print(f"MÃ©thode: {method}")
print(f"Articles: {count}")
print(message)
```

---

## ğŸ”§ Scraping PersonnalisÃ©

### WordPress API Directement

```python
from database.db_manager import DatabaseManager
from scrapers.wordpress_scraper import WordPressScraper

db = DatabaseManager()

# CrÃ©er le scraper
scraper = WordPressScraper('https://lefaso.net')

# VÃ©rifier si c'est WordPress
if scraper.is_wordpress():
    print("âœ… WordPress dÃ©tectÃ©")
    
    # Ajouter le mÃ©dia
    media_id = db.add_media('Lefaso.net', 'https://lefaso.net', 'wordpress')
    
    # Scraper
    articles = scraper.scrape(media_id, days=30)
    
    # Sauvegarder
    for article in articles:
        db.add_article(article)
    
    print(f"âœ… {len(articles)} articles collectÃ©s")
else:
    print("âŒ Pas WordPress")
```

### HTML Scraping Directement

```python
from database.db_manager import DatabaseManager
from scrapers.html_scraper import HTMLScraper

db = DatabaseManager()

# CrÃ©er le scraper
scraper = HTMLScraper('https://www.aib.media')

# Ajouter le mÃ©dia
media_id = db.add_media('AIB', 'https://www.aib.media', 'html')

# Scraper (max 20 articles)
articles = scraper.scrape(media_id, days=30, max_articles=20)

# Sauvegarder
for article in articles:
    db.add_article(article)

print(f"âœ… {len(articles)} articles collectÃ©s")
```

---

## ğŸ“Š AccÃ¨s aux DonnÃ©es

### RÃ©cupÃ©rer les Articles RÃ©cents

```python
from database.db_manager import DatabaseManager

db = DatabaseManager()

# Articles des 7 derniers jours
articles = db.get_recent_articles(days=7, limit=50)

for article in articles:
    print(f"Titre: {article.titre}")
    print(f"MÃ©dia: {article.media_id}")
    print(f"Date: {article.date_publication}")
    print(f"URL: {article.url}")
    print(f"Contenu: {article.contenu[:200]}...")
    print("-" * 60)
```

### RÃ©cupÃ©rer les Articles d'un MÃ©dia

```python
from database.db_manager import DatabaseManager

db = DatabaseManager()

# RÃ©cupÃ©rer le mÃ©dia
media = db.get_media_by_url('https://lefaso.net')

if media:
    # Articles de ce mÃ©dia
    articles = db.get_articles_by_media(media.id, limit=100)
    
    print(f"ğŸ“° {len(articles)} articles de {media.nom}")
    
    for article in articles:
        print(f"- {article.titre}")
```

### VÃ©rifier si un Article Existe

```python
from database.db_manager import DatabaseManager

db = DatabaseManager()

url = "https://lefaso.net/spip.php?article123456"

if db.article_exists(url):
    print("âœ… Article dÃ©jÃ  en base")
else:
    print("âŒ Article non trouvÃ©")
```

---

## ğŸ“ˆ Statistiques

### Statistiques Globales

```python
from database.db_manager import DatabaseManager

db = DatabaseManager()

stats = db.get_scraping_stats()

print(f"ğŸ“° Total articles: {stats['total_articles']}")
print(f"\nğŸ“º Articles par mÃ©dia:")
for media, count in stats['articles_par_media'].items():
    print(f"   â€¢ {media}: {count} articles")

print(f"\nğŸ”§ Articles par source:")
for source, count in stats['articles_par_source'].items():
    print(f"   â€¢ {source}: {count} articles")

print(f"\nğŸ“‹ Derniers logs:")
for log in stats['derniers_logs'][:5]:
    print(f"   â€¢ {log['media_nom']}: {log['articles_collectes']} articles ({log['status']})")
```

### Compter les Articles

```python
from database.db_manager import DatabaseManager

db = DatabaseManager()

# Total
total = db.get_article_count()
print(f"Total articles: {total}")

# Par mÃ©dia
media = db.get_media_by_url('https://lefaso.net')
if media:
    count = db.get_article_count(media_id=media.id)
    print(f"Articles de {media.nom}: {count}")
```

### Tous les MÃ©dias Actifs

```python
from database.db_manager import DatabaseManager

db = DatabaseManager()

medias = db.get_all_active_medias()

print(f"ğŸ“º {len(medias)} mÃ©dias actifs:")
for media in medias:
    print(f"   â€¢ {media.nom} ({media.type_site})")
    print(f"     URL: {media.url}")
    print(f"     DerniÃ¨re collecte: {media.derniere_collecte}")
```

---

## ğŸ¤– Automatisation

### Script de Scraping Quotidien

```python
#!/usr/bin/env python3
# scrape_daily.py

from database.db_manager import DatabaseManager
from scrapers.scraper_manager import ScraperManager
from datetime import datetime

def main():
    print(f"ğŸš€ Scraping quotidien - {datetime.now()}")
    
    # Initialiser
    db = DatabaseManager()
    manager = ScraperManager(db)
    
    # Scraper seulement les nouveaux articles (1 jour)
    stats = manager.scrape_all_sites(
        sites_file='sites.txt',
        days=1
    )
    
    print(f"\nâœ… {stats['total_articles']} nouveaux articles collectÃ©s")
    
    # Envoyer un email de rapport (optionnel)
    # send_report_email(stats)

if __name__ == '__main__':
    main()
```

**Automatisation Windows (Planificateur de tÃ¢ches) :**

```batch
@echo off
cd C:\Users\DarkSide\Desktop\Media_Scanne\backend\django_back
python scrape_daily.py >> logs\scraping.log 2>&1
```

**Automatisation Linux/Mac (crontab) :**

```bash
# Tous les jours Ã  6h du matin
0 6 * * * cd /path/to/django_back && python scrape_daily.py >> logs/scraping.log 2>&1
```

### Nettoyage Automatique des Anciens Articles

```python
from database.db_manager import DatabaseManager

db = DatabaseManager()

# Supprimer les articles de plus de 90 jours
deleted = db.clear_old_articles(days=90)
print(f"ğŸ—‘ï¸ {deleted} anciens articles supprimÃ©s")

# Optimiser la base de donnÃ©es
db.vacuum()
print("âœ… Base de donnÃ©es optimisÃ©e")
```

---

## ğŸ” Recherche et Filtrage

### Recherche par Mot-ClÃ© (Simple)

```python
from database.db_manager import DatabaseManager
import sqlite3

db = DatabaseManager()
conn = db.get_connection()
cursor = conn.cursor()

# Recherche dans le titre ou le contenu
keyword = "politique"

cursor.execute("""
    SELECT titre, url, date_publication
    FROM articles
    WHERE titre LIKE ? OR contenu LIKE ?
    ORDER BY date_publication DESC
    LIMIT 20
""", (f'%{keyword}%', f'%{keyword}%'))

results = cursor.fetchall()

print(f"ğŸ” {len(results)} articles trouvÃ©s pour '{keyword}':")
for titre, url, date in results:
    print(f"   â€¢ {titre}")
    print(f"     {url}")
    print(f"     {date}")
    print()

conn.close()
```

### Filtrer par CatÃ©gorie

```python
from database.db_manager import DatabaseManager
import sqlite3
import json

db = DatabaseManager()
conn = db.get_connection()
cursor = conn.cursor()

# Articles de la catÃ©gorie "Politique"
cursor.execute("""
    SELECT titre, url, categories
    FROM articles
    WHERE categories LIKE '%Politique%'
    ORDER BY date_publication DESC
    LIMIT 20
""")

results = cursor.fetchall()

print(f"ğŸ“° {len(results)} articles de catÃ©gorie 'Politique':")
for titre, url, categories in results:
    print(f"   â€¢ {titre}")
    print(f"     CatÃ©gories: {categories}")
    print()

conn.close()
```

---

## ğŸ“¤ Export de DonnÃ©es

### Export CSV

```python
from database.db_manager import DatabaseManager
import csv

db = DatabaseManager()

# RÃ©cupÃ©rer les articles
articles = db.get_recent_articles(days=30, limit=1000)

# Export CSV
with open('articles_export.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    
    # En-tÃªtes
    writer.writerow(['Titre', 'URL', 'Date', 'Auteur', 'Source'])
    
    # DonnÃ©es
    for article in articles:
        writer.writerow([
            article.titre,
            article.url,
            article.date_publication,
            article.auteur or '',
            article.source_type
        ])

print("âœ… Export CSV terminÃ©: articles_export.csv")
```

### Export JSON

```python
from database.db_manager import DatabaseManager
import json

db = DatabaseManager()

# RÃ©cupÃ©rer les articles
articles = db.get_recent_articles(days=30, limit=1000)

# Convertir en dictionnaires
articles_dict = []
for article in articles:
    articles_dict.append({
        'titre': article.titre,
        'url': article.url,
        'date_publication': str(article.date_publication),
        'auteur': article.auteur,
        'contenu': article.contenu,
        'source_type': article.source_type
    })

# Export JSON
with open('articles_export.json', 'w', encoding='utf-8') as f:
    json.dump(articles_dict, f, ensure_ascii=False, indent=2)

print("âœ… Export JSON terminÃ©: articles_export.json")
```

---

## ğŸ§ª Tests et Validation

### Tester un Site Avant de l'Ajouter

```python
from scrapers.wordpress_scraper import WordPressScraper
from scrapers.html_scraper import HTMLScraper

url = "https://nouveau-site.bf"

# Test WordPress
print(f"ğŸ” Test de {url}...")
wp_scraper = WordPressScraper(url)

if wp_scraper.is_wordpress():
    print("âœ… WordPress dÃ©tectÃ© - RecommandÃ©")
else:
    print("âš ï¸ WordPress non dÃ©tectÃ© - HTML scraping sera utilisÃ©")
    
    # Test HTML
    html_scraper = HTMLScraper(url)
    soup = html_scraper.get_page(url)
    
    if soup:
        links = html_scraper.find_article_links(soup)
        print(f"   {len(links)} liens d'articles trouvÃ©s")
        
        if len(links) > 0:
            print("âœ… HTML scraping possible")
        else:
            print("âŒ Aucun article trouvÃ© - Structure HTML non compatible")
    else:
        print("âŒ Impossible de rÃ©cupÃ©rer la page")
```

### Valider les DonnÃ©es CollectÃ©es

```python
from database.db_manager import DatabaseManager

db = DatabaseManager()

# VÃ©rifier les articles sans contenu
conn = db.get_connection()
cursor = conn.cursor()

cursor.execute("""
    SELECT COUNT(*) as count
    FROM articles
    WHERE contenu IS NULL OR LENGTH(contenu) < 100
""")

invalid_count = cursor.fetchone()['count']

if invalid_count > 0:
    print(f"âš ï¸ {invalid_count} articles avec contenu invalide")
else:
    print("âœ… Tous les articles ont un contenu valide")

conn.close()
```

---

## ğŸ¨ Personnalisation

### Ajouter un Nouveau Scraper

```python
# scrapers/custom_scraper.py

from typing import List
from database.models import Article
from datetime import datetime

class CustomScraper:
    """Scraper personnalisÃ© pour un site spÃ©cifique"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
    
    def scrape(self, media_id: int, days: int = 30) -> List[Article]:
        """
        Scraper personnalisÃ©
        
        Returns:
            Liste d'objets Article
        """
        articles = []
        
        # Votre logique de scraping ici
        # ...
        
        return articles
```

### Modifier les SÃ©lecteurs HTML

```python
# Dans scrapers/html_scraper.py

def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
    # Ajouter vos sÃ©lecteurs personnalisÃ©s
    selectors = [
        'h1.mon-selecteur-custom',  # Votre sÃ©lecteur
        'h1.entry-title',
        'h1.post-title',
        # ...
    ]
    
    for selector in selectors:
        elem = soup.select_one(selector)
        if elem:
            return elem.get_text(strip=True)
    
    return None
```

---

## ğŸ’¡ Conseils et Bonnes Pratiques

### 1. Gestion des Erreurs

```python
from database.db_manager import DatabaseManager
from scrapers.scraper_manager import ScraperManager

db = DatabaseManager()
manager = ScraperManager(db)

try:
    count, method, message = manager.scrape_site('https://lefaso.net', days=30)
    print(f"âœ… SuccÃ¨s: {count} articles")
except Exception as e:
    print(f"âŒ Erreur: {e}")
    # Logger l'erreur
    import traceback
    traceback.print_exc()
```

### 2. Scraping Progressif

```python
# Scraper par petits lots pour Ã©viter les timeouts
urls = [
    'https://lefaso.net',
    'https://www.sidwaya.info',
    'https://www.fasopresse.net',
    # ...
]

for url in urls:
    try:
        print(f"\nğŸ¯ Scraping {url}...")
        count, method, message = manager.scrape_site(url, days=7)
        print(message)
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        continue  # Continuer avec le site suivant
```

### 3. Monitoring

```python
from database.db_manager import DatabaseManager
from datetime import datetime

db = DatabaseManager()

# VÃ©rifier la derniÃ¨re collecte de chaque mÃ©dia
medias = db.get_all_active_medias()

print("ğŸ“Š Ã‰tat des mÃ©dias:")
for media in medias:
    if media.derniere_collecte:
        print(f"   â€¢ {media.nom}: {media.derniere_collecte}")
    else:
        print(f"   â€¢ {media.nom}: âŒ Jamais scrapÃ©")
```

---

## ğŸ¯ Cas d'Usage AvancÃ©s

### Dashboard Simple en Console

```python
from database.db_manager import DatabaseManager
from datetime import datetime

def show_dashboard():
    db = DatabaseManager()
    stats = db.get_scraping_stats()
    
    print("\n" + "="*60)
    print("ğŸ“Š MÃ‰DIA-SCAN DASHBOARD")
    print("="*60)
    print(f"\nğŸ“° Total articles: {stats['total_articles']}")
    
    print(f"\nğŸ“º Top 5 mÃ©dias:")
    sorted_medias = sorted(
        stats['articles_par_media'].items(),
        key=lambda x: x[1],
        reverse=True
    )
    for i, (media, count) in enumerate(sorted_medias[:5], 1):
        print(f"   {i}. {media}: {count} articles")
    
    print(f"\nğŸ”§ MÃ©thodes de scraping:")
    for method, count in stats['articles_par_source'].items():
        print(f"   â€¢ {method}: {count} articles")
    
    print(f"\nğŸ“… DerniÃ¨re mise Ã  jour: {datetime.now()}")
    print("="*60)

if __name__ == '__main__':
    show_dashboard()
```

---

**Pour plus d'exemples, consultez les scripts `run_scraper.py` et `test_scraper.py` !**
