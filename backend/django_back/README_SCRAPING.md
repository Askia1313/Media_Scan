# ğŸ“° MÃ‰DIA-SCAN - SystÃ¨me de Scraping Intelligent

## ğŸ¯ Vue d'Ensemble

SystÃ¨me de scraping automatique pour collecter les articles des mÃ©dias burkinabÃ¨ avec :
- âœ… **DÃ©tection automatique WordPress** (API REST)
- âœ… **Fallback HTML** si WordPress non disponible
- âœ… **Sauvegarde SQLite** avec gestion des doublons
- âœ… **Collecte des 30 derniers jours**
- âœ… **Multi-sites** via fichier de configuration

---

## ğŸ—ï¸ Architecture

```
django_back/
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py           # ModÃ¨les Article et Media
â”‚   â”œâ”€â”€ schema.sql          # SchÃ©ma SQLite
â”‚   â””â”€â”€ db_manager.py       # Gestionnaire de base de donnÃ©es
â”‚
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ wordpress_scraper.py    # Scraper WordPress API
â”‚   â”œâ”€â”€ html_scraper.py         # Scraper HTML (fallback)
â”‚   â””â”€â”€ scraper_manager.py      # Gestionnaire principal
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ text_utils.py       # Utilitaires texte
â”‚   â””â”€â”€ date_utils.py       # Utilitaires dates
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ media_scan.db       # Base de donnÃ©es SQLite (crÃ©Ã©e auto)
â”‚
â”œâ”€â”€ sites.txt               # Liste des sites Ã  scraper
â”œâ”€â”€ run_scraper.py          # Script principal
â”œâ”€â”€ test_scraper.py         # Script de test
â””â”€â”€ requirements.txt        # DÃ©pendances
```

---

## ğŸ“‹ FonctionnalitÃ©s

### 1. Scraping WordPress (PrioritÃ©)

Le systÃ¨me dÃ©tecte automatiquement si un site utilise WordPress et utilise l'API REST :

**Avantages :**
- âœ… DonnÃ©es structurÃ©es (titre, contenu, date, auteur, catÃ©gories, tags)
- âœ… Rapide et fiable
- âœ… Pas de parsing HTML complexe
- âœ… MÃ©tadonnÃ©es complÃ¨tes

**DonnÃ©es rÃ©cupÃ©rÃ©es :**
- Titre de l'article
- Contenu complet (texte)
- Extrait
- URL
- Auteur
- Date de publication
- Image Ã  la une
- CatÃ©gories
- Tags
- Nombre de commentaires

### 2. Scraping HTML (Fallback)

Si WordPress n'est pas disponible, le systÃ¨me bascule automatiquement sur le scraping HTML :

**Fonctionnement :**
- RÃ©cupÃ©ration de la page d'accueil
- DÃ©tection automatique des liens d'articles
- Extraction du contenu via sÃ©lecteurs CSS gÃ©nÃ©riques
- Parsing intelligent du titre, contenu, date, auteur

**Limitations :**
- Moins de mÃ©tadonnÃ©es
- DÃ©pend de la structure HTML du site
- Plus lent

### 3. Base de DonnÃ©es SQLite

**Tables :**
- `medias` : Liste des mÃ©dias scrapÃ©s
- `articles` : Articles collectÃ©s
- `scraping_logs` : Logs de scraping

**FonctionnalitÃ©s :**
- Gestion automatique des doublons (par URL)
- Index pour performances
- Statistiques de collecte
- Nettoyage des anciens articles

---

## ğŸš€ Installation

### 1. PrÃ©requis

```bash
Python 3.9+
```

### 2. Installer les dÃ©pendances

```bash
cd django_back
pip install -r requirements.txt
```

**DÃ©pendances principales :**
- `requests` : RequÃªtes HTTP
- `beautifulsoup4` : Parsing HTML
- `lxml` : Parser XML/HTML rapide

---

## ğŸ“ Configuration

### Fichier `sites.txt`

CrÃ©ez ou modifiez le fichier `sites.txt` avec les URLs des sites Ã  scraper :

```txt
# MÃ©dias burkinabÃ¨
https://lefaso.net
https://www.sidwaya.info
https://www.fasopresse.net
https://www.lobservateur.bf
https://www.aib.media

# Autres mÃ©dias (dÃ©commentez si nÃ©cessaire)
# https://www.burkina24.com
# https://www.lepays.bf
```

**Format :**
- Une URL par ligne
- Les lignes commenÃ§ant par `#` sont des commentaires
- Les lignes vides sont ignorÃ©es

---

## ğŸ® Utilisation

### 1. Scraper tous les sites

```bash
python run_scraper.py
```

**Options :**
```bash
# SpÃ©cifier le nombre de jours
python run_scraper.py --days 30

# Utiliser un autre fichier de sites
python run_scraper.py --sites-file mes_sites.txt

# SpÃ©cifier le chemin de la base de donnÃ©es
python run_scraper.py --db-path data/custom.db
```

### 2. Scraper un seul site

```bash
python run_scraper.py --url https://lefaso.net
```

### 3. Afficher les statistiques

```bash
python run_scraper.py --stats
```

### 4. Tester le systÃ¨me

```bash
python test_scraper.py
```

**Tests effectuÃ©s :**
1. DÃ©tection WordPress sur plusieurs sites
2. Scraping WordPress avec sauvegarde
3. Scraping HTML avec sauvegarde
4. OpÃ©rations de base de donnÃ©es

---

## ğŸ“Š Exemple de Sortie

```
============================================================
ğŸš€ MÃ‰DIA-SCAN - Collecte Multi-Sites
============================================================

ğŸ“‹ 5 sites Ã  scraper
ğŸ“… PÃ©riode: 30 derniers jours

============================================================
ğŸ¯ Scraping: Lefaso (https://lefaso.net)
============================================================

ğŸ“¡ Tentative 1: API WordPress...
âœ… Site WordPress dÃ©tectÃ©!
ğŸ“¡ RÃ©cupÃ©ration articles WordPress depuis https://lefaso.net...
   Page 1: 100 articles rÃ©cupÃ©rÃ©s
   Page 2: 100 articles rÃ©cupÃ©rÃ©s
   Page 3: 45 articles rÃ©cupÃ©rÃ©s
âœ… Total: 245 articles rÃ©cupÃ©rÃ©s
âœ… 245 articles collectÃ©s via API WordPress

[2/5] Traitement de https://www.sidwaya.info...
...

============================================================
ğŸ“Š RÃ‰SUMÃ‰ DE LA COLLECTE
============================================================

âœ… Sites traitÃ©s: 5
   â€¢ SuccÃ¨s: 4
   â€¢ Erreurs: 1

ğŸ“° Total articles collectÃ©s: 487

ğŸ”§ Par mÃ©thode:
   â€¢ WordPress API: 412 articles
   â€¢ HTML Scraping: 75 articles

ğŸ“‹ DÃ©tails par site:
   âœ… https://lefaso.net: 245 articles (wordpress_api)
   âœ… https://www.sidwaya.info: 167 articles (wordpress_api)
   âœ… https://www.fasopresse.net: 89 articles (html_scraping)
   âœ… https://www.aib.media: 56 articles (html_scraping)
   âŒ https://www.lobservateur.bf: 0 articles (error)
```

---

## ğŸ”§ API Python

### Utilisation programmatique

```python
from database.db_manager import DatabaseManager
from scrapers.scraper_manager import ScraperManager

# Initialiser
db = DatabaseManager('data/media_scan.db')
manager = ScraperManager(db)

# Scraper un site
count, method, message = manager.scrape_site('https://lefaso.net', days=30)
print(f"CollectÃ©: {count} articles via {method}")

# Scraper tous les sites
stats = manager.scrape_all_sites('sites.txt', days=30)
print(f"Total: {stats['total_articles']} articles")

# RÃ©cupÃ©rer les articles
articles = db.get_recent_articles(days=30, limit=100)
for article in articles:
    print(f"{article.titre} - {article.date_publication}")

# Statistiques
stats = db.get_scraping_stats()
print(f"Total articles: {stats['total_articles']}")
```

---

## ğŸ“¦ Structure de DonnÃ©es

### ModÃ¨le Article

```python
@dataclass
class Article:
    id: Optional[int]
    media_id: int
    titre: str
    contenu: str
    extrait: str
    url: str
    auteur: Optional[str]
    date_publication: Optional[datetime]
    image_url: Optional[str]
    categories: Optional[str]  # JSON
    tags: Optional[str]  # JSON
    source_type: str  # 'wordpress_api' ou 'html_scraping'
    scraped_at: Optional[datetime]
    vues: int
    commentaires: int
```

### ModÃ¨le Media

```python
@dataclass
class Media:
    id: Optional[int]
    nom: str
    url: str
    type_site: str  # 'wordpress', 'html', 'unknown'
    actif: bool
    derniere_collecte: Optional[datetime]
```

---

## ğŸ› ï¸ Personnalisation

### Ajouter un nouveau site

1. Ajoutez l'URL dans `sites.txt`
2. Lancez le scraper : `python run_scraper.py`

Le systÃ¨me dÃ©tectera automatiquement la meilleure mÃ©thode.

### Modifier les sÃ©lecteurs HTML

Si le scraping HTML ne fonctionne pas pour un site spÃ©cifique, modifiez `scrapers/html_scraper.py` :

```python
def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
    # Ajoutez vos sÃ©lecteurs personnalisÃ©s
    selectors = [
        'h1.custom-title',  # Votre sÃ©lecteur
        'h1.entry-title',
        # ...
    ]
```

### Changer la pÃ©riode de collecte

```bash
# 7 derniers jours
python run_scraper.py --days 7

# 90 derniers jours
python run_scraper.py --days 90
```

---

## ğŸ› DÃ©pannage

### Erreur : "Site non WordPress ou API non accessible"

**Cause :** Le site n'utilise pas WordPress ou l'API est dÃ©sactivÃ©e.

**Solution :** Le systÃ¨me bascule automatiquement sur le scraping HTML.

### Erreur : "Aucun article trouvÃ©"

**Causes possibles :**
1. Le site a changÃ© sa structure HTML
2. Le site bloque les scrapers
3. ProblÃ¨me de connexion

**Solutions :**
1. VÃ©rifiez votre connexion internet
2. Testez manuellement l'URL dans un navigateur
3. Modifiez les sÃ©lecteurs CSS dans `html_scraper.py`

### Erreur : "Database locked"

**Cause :** Plusieurs processus accÃ¨dent Ã  la base de donnÃ©es simultanÃ©ment.

**Solution :** Attendez que le scraping en cours se termine.

### Articles dupliquÃ©s

**Impossible :** Le systÃ¨me utilise l'URL comme clÃ© unique (contrainte UNIQUE).

---

## ğŸ“ˆ Performance

### Vitesse de scraping

- **WordPress API** : ~100 articles/minute
- **HTML Scraping** : ~10-20 articles/minute (avec pauses)

### Recommandations

1. **Pauses entre requÃªtes** : Le systÃ¨me inclut des pauses automatiques (1-2s)
2. **Limiter le nombre d'articles** : Utilisez `--days` pour limiter la pÃ©riode
3. **Scraping progressif** : Scrapez rÃ©guliÃ¨rement (ex: tous les jours) plutÃ´t que tout d'un coup

---

## ğŸ”’ ConsidÃ©rations LÃ©gales

### Respect des sites

- âœ… Scraping de contenu public uniquement
- âœ… Pauses entre requÃªtes pour ne pas surcharger les serveurs
- âœ… User-Agent identifiable
- âœ… Respect du robots.txt (Ã  implÃ©menter si nÃ©cessaire)

### Utilisation des donnÃ©es

- Les donnÃ©es collectÃ©es sont Ã  usage d'analyse et de recherche
- Respectez les droits d'auteur des contenus
- Ne republiez pas les contenus sans autorisation

---

## ğŸš€ Prochaines Ã‰tapes

### AmÃ©liorations possibles

1. **Respect du robots.txt** : VÃ©rifier automatiquement
2. **Scraping incrÃ©mental** : Ne rÃ©cupÃ©rer que les nouveaux articles
3. **DÃ©tection de langue** : Filtrer par langue franÃ§aise
4. **Classification automatique** : CatÃ©goriser les articles (ML)
5. **API REST** : Exposer les donnÃ©es via API
6. **Dashboard** : Interface web pour visualiser les donnÃ©es
7. **Notifications** : Alertes pour nouveaux articles importants
8. **Export** : CSV, JSON, Excel

---

## ğŸ“ Support

### Logs

Les logs de scraping sont sauvegardÃ©s dans la table `scraping_logs` :

```python
# Afficher les derniers logs
python run_scraper.py --stats
```

### Debug

Pour activer le mode debug, modifiez le code :

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

## ğŸ“„ Licence

Ce projet est dÃ©veloppÃ© dans le cadre du Hackathon MTDPCE AI 2025.

---

## âœ… Checklist de DÃ©marrage

- [ ] Python 3.9+ installÃ©
- [ ] DÃ©pendances installÃ©es (`pip install -r requirements.txt`)
- [ ] Fichier `sites.txt` configurÃ©
- [ ] Tests exÃ©cutÃ©s (`python test_scraper.py`)
- [ ] Premier scraping lancÃ© (`python run_scraper.py`)
- [ ] Statistiques vÃ©rifiÃ©es (`python run_scraper.py --stats`)

**PrÃªt Ã  scraper ! ğŸš€**
