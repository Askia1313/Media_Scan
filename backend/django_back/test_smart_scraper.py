#!/usr/bin/env python3
"""Test du scraper HTML intelligent"""

from scrapers.smart_html_scraper import SmartHTMLScraper
from bs4 import BeautifulSoup

url = "https://lepays.bf"

print(f"ðŸ” Test du scraper intelligent: {url}\n")

scraper = SmartHTMLScraper(url)

# RÃ©cupÃ©rer la page
soup = scraper.get_page(url)

if not soup:
    print("âŒ Impossible de rÃ©cupÃ©rer la page")
    exit(1)

print(f"âœ… Page rÃ©cupÃ©rÃ©e\n")

# Tester les sÃ©lecteurs
selectors = [
    'article.post a[href]',
    'article a.entry-title[href]',
    '.post-item a[href]',
    '.entry-title a[href]',
    'article h2 a[href]',
    'h2 a[href]',
    'h3 a[href]',
    'a[href*="/202"]',
]

print("ðŸ“‹ Test des sÃ©lecteurs:\n")

for selector in selectors:
    elements = soup.select(selector)
    print(f"   {selector}: {len(elements)} Ã©lÃ©ments")
    if len(elements) > 0 and len(elements) <= 3:
        for elem in elements[:2]:
            href = elem.get('href')
            if href:
                from urllib.parse import urljoin
                full_url = urljoin(url, href)
                is_article = scraper._is_article_url(full_url)
                print(f"      â†’ {full_url[:80]}... [Article: {is_article}]")

print("\n" + "="*60)
print("ðŸ” Recherche Ã©largie de tous les liens:\n")

all_links = soup.find_all('a', href=True)
print(f"Total liens trouvÃ©s: {len(all_links)}")

article_links = []
for link in all_links[:50]:  # Tester les 50 premiers
    href = link.get('href')
    from urllib.parse import urljoin, urlparse
    full_url = urljoin(url, href)
    
    # VÃ©rifier que c'est un lien interne
    if urlparse(full_url).netloc == urlparse(url).netloc:
        if scraper._is_article_url(full_url):
            article_links.append(full_url)

print(f"\nLiens d'articles trouvÃ©s: {len(article_links)}")
print("\nðŸ“° Premiers 10 liens d'articles:\n")

for i, link in enumerate(article_links[:10], 1):
    print(f"   {i}. {link}")
